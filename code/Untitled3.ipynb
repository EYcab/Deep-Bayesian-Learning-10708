{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TensorArrayReadV3:0\", shape=(?, 20), dtype=float32)\n",
      "Tensor(\"add_8:0\", shape=(5, 2), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(5, 20), dtype=float32)\n",
      "Tensor(\"transpose_3:0\", shape=(5, ?, 20), dtype=float32)\n",
      "Tensor(\"Gather_1:0\", shape=(?, 10, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq, SimpleSeq2Seq, Seq2Seq\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# with open(\"word_index_conversion.p\",'rb') as f:\n",
    "#     word_index_conversion = pickle.load(f)\n",
    "    \n",
    "# word2index = word_index_conversion['word2index']\n",
    "# index2word = word_index_conversion['index2word']\n",
    "\n",
    "# lookup_matrix = np.load('word2vec.npy')\n",
    "\n",
    "lookup_matrix = np.zeros((100,20))\n",
    "\n",
    "# sentences = np.load('sentences_as_list.npy')\n",
    "\n",
    "# filtered_sentences = [filter(lambda x:word2index.has_key(x), s) for s in sentences]\n",
    "# # Filter out sentences with length larger than 2\n",
    "# filtered_sentences = filter(lambda x:len(x) > 2, filtered_sentences)\n",
    "\n",
    "# idxes = [[word2index[w] for w in s] for s in filtered_sentences]\n",
    "\n",
    "# Constants\n",
    "batch_size = 5\n",
    "time_steps = 10\n",
    "embedding_size = lookup_matrix.shape[1]\n",
    "latent_dim = 2\n",
    "hidden_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "time_steps = 10\n",
    "epsilon_std = 1\n",
    "\n",
    "# padded_idxes = pad_sequences(idxes, maxlen=time_steps, dtype='int32', \\\n",
    "#                                  padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "# x_train, x_test = train_test_split(padded_idxes, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train, x_test = np.zeros((30,time_steps)), np.zeros((10,time_steps))\n",
    "\n",
    "optimizer, loss, x, y = SimpleSeq2Seq(output_dim=embedding_size, output_length=time_steps, latent_dim=latent_dim, \\\n",
    "                                      batch_size=batch_size, epsilon_std=epsilon_std, lookup_matrix=lookup_matrix, \\\n",
    "                                      input_shape=(time_steps, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    epochs = 0\n",
    "    print_freq = 15\n",
    "    while epochs < 30:\n",
    "        print \"Epochs:\" + str(epochs)\n",
    "        i = 0\n",
    "        while i < len(x_train):\n",
    "            samples = x_train[i:i+batch_size,:]\n",
    "            i += batch_size\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={x:samples, y:samples})\n",
    "            if i % print_freq == 0:\n",
    "                print loss_val\n",
    "        epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SimpleSeq2Seq(output_dim=embedding_size, output_length=time_steps, input_shape=(time_steps, embedding_size))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train_embedded, x_train_embedded, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 2\n",
    "\n",
    "output_length = 8\n",
    "output_dim = 3\n",
    "\n",
    "samples = 100\n",
    "\n",
    "x = np.random.random((samples, input_length, input_dim))\n",
    "y = np.random.random((samples, output_length, output_dim))\n",
    "\n",
    "lookup_matrix\n",
    "\n",
    "models = []\n",
    "models += [SimpleSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim))]\n",
    "models += [SimpleSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim), depth=2)]\n",
    "\n",
    "for model in models:\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    model.fit(x, y, nb_epoch=1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='sgd')\n",
    "model.fit(x_train, x_train, nb_epoch=1, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 2\n",
    "\n",
    "output_length = 8\n",
    "output_dim = 3\n",
    "\n",
    "samples = 100\n",
    "\n",
    "model = AttentionSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(batch_size, time_steps))\n",
    "x = Embedding(input_dim=lookup_matrix.shape[0], output_dim=lookup_matrix.shape[1], weights=[lookup_matrix])(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "h_encoded = LSTM(hidden_dim, batch_input_shape=(batch_size, time_steps, embedding_size), \\\n",
    "                     return_sequences=False, stateful=True)(x)\n",
    "\n",
    "z_mean = Dense(latent_dim)(h_encoded)\n",
    "z_log_var = Dense(latent_dim)(h_encoded)\n",
    "\n",
    "# # build a model to project inputs on the latent space\n",
    "# encoder = Model(x, z_mean)\n",
    "\n",
    "# # display a 2D plot of the digit classes in the latent space\n",
    "# x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# # build a digit generator that can sample from the learned distribution\n",
    "# decoder_input = Input(shape=(latent_dim,))\n",
    "# _h_decoded = decoder_h(decoder_input)\n",
    "# _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "# generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# # display a 2D manifold of the digits\n",
    "# n = 15  # figure with 15x15 digits\n",
    "# digit_size = 28\n",
    "# figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# # to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "# grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "# grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "# for i, yi in enumerate(grid_x):\n",
    "#     for j, xi in enumerate(grid_y):\n",
    "#         z_sample = np.array([[xi, yi]])\n",
    "#         x_decoded = generator.predict(z_sample)\n",
    "#         digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "#         figure[i * digit_size: (i + 1) * digit_size,\n",
    "#                j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(figure, cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args         \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "h_0 = Dense(hidden_dim, activation='relu')(z)\n",
    "h_decoded = LSTM(embedding_size, batch_input_shape=(batch_size, time_steps, hidden_dim), \\\n",
    "                     return_sequences=True, stateful=True)(h_0)\n",
    "\n",
    "x_decoded_mean = Dense(original_dim, activation='sigmoid')(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(36,))\n",
    "x = Embedding(input_dim=lookup_matrix.shape[0], output_dim=lookup_matrix.shape[1], weights=[lookup_matrix])(input_tensor)\n",
    "\n",
    "model = Model(input=[input_tensor], output=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess.run(init)\n",
    "\n",
    "temp = sess.run(h_encoded, feed_dict={model.input:padded_idxes[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, LSTM\n",
    "\n",
    "batch_size = 10\n",
    "time_steps = 36\n",
    "embedding_size = 300\n",
    "\n",
    "h_encoded = LSTM(10, input_shape=(time_steps, embedding_size), \\\n",
    "                     return_sequences=True, stateful=False)(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
