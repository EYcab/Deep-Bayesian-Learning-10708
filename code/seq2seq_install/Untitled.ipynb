{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "\n",
    "from seq2seq.models import AttentionSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args         \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "def AttentionSeq2Seq(output_dim, output_length, hidden_dim=None, depth=1, bidirectional=True, dropout=0., **kwargs):\n",
    "    '''\n",
    "    This is an attention Seq2seq model with variational bound\n",
    "\n",
    "    '''\n",
    "    if type(depth) == int:\n",
    "        depth = [depth, depth]\n",
    "    if 'batch_input_shape' in kwargs:\n",
    "        shape = kwargs['batch_input_shape']\n",
    "        del kwargs['batch_input_shape']\n",
    "    elif 'input_shape' in kwargs:\n",
    "        shape = (None,) + tuple(kwargs['input_shape'])\n",
    "        del kwargs['input_shape']\n",
    "    elif 'input_dim' in kwargs:\n",
    "        if 'input_length' in kwargs:\n",
    "            shape = (None, kwargs['input_length'], kwargs['input_dim'])\n",
    "            del kwargs['input_length']\n",
    "        else:\n",
    "            shape = (None, None, kwargs['input_dim'])\n",
    "        del kwargs['input_dim']\n",
    "    if 'unroll' in kwargs:\n",
    "        unroll = kwargs['unroll']\n",
    "        del kwargs['unroll']\n",
    "    else:\n",
    "        unroll = False\n",
    "    if 'stateful' in kwargs:\n",
    "        stateful = kwargs['stateful']\n",
    "        del kwargs['stateful']\n",
    "    else:\n",
    "        stateful = False\n",
    "    if not hidden_dim:\n",
    "        hidden_dim = output_dim\n",
    "        \n",
    "    encoder = RecurrentContainer(unroll=unroll, stateful=stateful, return_sequences=True, input_length=shape[1])\n",
    "    encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], shape[2]), **kwargs))\n",
    "    for _ in range(1, depth[0]):\n",
    "        encoder.add(Dropout(dropout))\n",
    "        encoder.add(LSTMCell(hidden_dim, **kwargs))\n",
    "    input = Input(batch_shape=shape)\n",
    "    input._keras_history[0].supports_masking = True\n",
    "    if bidirectional:\n",
    "        encoder = Bidirectional(encoder, merge_mode='sum')\n",
    "        \n",
    "    h_encoded = encoder(input)\n",
    "    \n",
    "    # Variational part\n",
    "    z_mean = Dense(latent_dim)(h_encoded)\n",
    "    z_log_var = Dense(latent_dim)(h_encoded)\n",
    "    \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "    \n",
    "    h_0 = Dense(hidden_dim, activation='relu')(z)\n",
    "    \n",
    "    decoder = RecurrentContainer(decode=True, output_length=output_length, unroll=unroll, stateful=stateful, input_length=shape[1])\n",
    "    decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "    if depth[1] == 1:\n",
    "        decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    else:\n",
    "        decoder.add(AttentionDecoderCell(output_dim=hidden_dim, hidden_dim=hidden_dim))\n",
    "        for _ in range(depth[1] - 2):\n",
    "            decoder.add(Dropout(dropout))\n",
    "            decoder.add(LSTMDecoderCell(output_dim=hidden_dim, hidden_dim=hidden_dim))\n",
    "        decoder.add(Dropout(dropout))\n",
    "        decoder.add(LSTMDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    inputs = [input]\n",
    "    '''\n",
    "    if teacher_force:\n",
    "        truth_tensor = Input(batch_shape=(shape[0], output_length, output_dim))\n",
    "        inputs += [truth_tensor]\n",
    "        decoder.set_truth_tensor(truth_tensor)\n",
    "    '''\n",
    "    x_decoded_mean = decoder(h_0)\n",
    "    \n",
    "    model = Model(inputs, decoded)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=vae_loss)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
