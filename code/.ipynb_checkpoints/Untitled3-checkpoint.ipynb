{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'word_index_conversion.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10f732cc2f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_index_conversion.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mword_index_conversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'word_index_conversion.p'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq, SimpleSeq2Seq, Seq2Seq\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "path = 'data/'\n",
    "\n",
    "with open(path+\"word_index_conversion.p\",'rb') as f:\n",
    "    word_index_conversion = pickle.load(f)\n",
    "    \n",
    "word2index = word_index_conversion['word2index']\n",
    "index2word = word_index_conversion['index2word']\n",
    "\n",
    "lookup_matrix = np.load(path+'word2vec.npy')\n",
    "\n",
    "# lookup_matrix = np.zeros((100,20))\n",
    "\n",
    "sentences = np.load(path+'sentences_as_list.npy')\n",
    "\n",
    "filtered_sentences = [filter(lambda x:word2index.has_key(x), s) for s in sentences]\n",
    "# Filter out sentences with length larger than 2\n",
    "filtered_sentences = filter(lambda x:len(x) > 2, filtered_sentences)\n",
    "\n",
    "idxes = [[word2index[w] for w in s] for s in filtered_sentences]\n",
    "\n",
    "# Constants\n",
    "batch_size = 5\n",
    "time_steps = 10\n",
    "embedding_size = lookup_matrix.shape[1]\n",
    "latent_dim = 2\n",
    "hidden_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "time_steps = 10\n",
    "epsilon_std = 1\n",
    "\n",
    "padded_idxes = pad_sequences(idxes, maxlen=time_steps, dtype='int32', \\\n",
    "                                 padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "x_train, x_test = train_test_split(padded_idxes, test_size=0.2, random_state=42)\n",
    "\n",
    "# x_train, x_test = np.zeros((50,time_steps)), np.zeros((10,time_steps))\n",
    "\n",
    "optimizer, loss, x, y = SimpleSeq2Seq(output_dim=embedding_size, output_length=time_steps, latent_dim=latent_dim, \\\n",
    "                                      batch_size=batch_size, epsilon_std=epsilon_std, lookup_matrix=lookup_matrix, \\\n",
    "                                      input_shape=(time_steps, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:0\n",
      "0.391779\n",
      "0.47102\n",
      "0.173725\n",
      "Epochs:1\n",
      "0.288921\n",
      "0.345377\n",
      "0.14924\n",
      "Epochs:2\n",
      "0.204905\n",
      "0.458748\n",
      "0.0476951\n",
      "Epochs:3\n",
      "0.103105\n",
      "0.114346\n",
      "0.130957\n",
      "Epochs:4\n",
      "0.09161\n",
      "0.0919707\n",
      "0.0538519\n",
      "Epochs:5\n",
      "0.0268746\n",
      "0.0663709\n",
      "0.0815156\n",
      "Epochs:6\n",
      "0.0676949\n",
      "0.0743142\n",
      "0.0720987\n",
      "Epochs:7\n",
      "0.059091\n",
      "0.0573937\n",
      "0.0479242\n",
      "Epochs:8\n",
      "0.0170523\n",
      "0.0285527\n",
      "0.0285718\n",
      "Epochs:9\n",
      "0.0221307\n",
      "0.0172562\n",
      "0.0120411\n",
      "Epochs:10\n",
      "0.0126128\n",
      "0.0133023\n",
      "0.0111356\n",
      "Epochs:11\n",
      "0.00620681\n",
      "0.00933929\n",
      "0.00592758\n",
      "Epochs:12\n",
      "0.00534937\n",
      "0.00616665\n",
      "0.00243882\n",
      "Epochs:13\n",
      "0.00842422\n",
      "0.00325086\n",
      "0.00545686\n",
      "Epochs:14\n",
      "0.0034179\n",
      "0.00223154\n",
      "0.0070753\n",
      "Epochs:15\n",
      "0.00278921\n",
      "0.00246248\n",
      "0.00367279\n",
      "Epochs:16\n",
      "0.00280396\n",
      "0.00200306\n",
      "0.00190948\n",
      "Epochs:17\n",
      "0.00189869\n",
      "0.00160371\n",
      "0.0019695\n",
      "Epochs:18\n",
      "0.00171516\n",
      "0.00189978\n",
      "0.00240907\n",
      "Epochs:19\n",
      "0.00433334\n",
      "0.0024307\n",
      "0.00167445\n",
      "Epochs:20\n",
      "0.00182351\n",
      "0.00224947\n",
      "0.00168242\n",
      "Epochs:21\n",
      "0.00197352\n",
      "0.00165952\n",
      "0.00166285\n",
      "Epochs:22\n",
      "0.00141207\n",
      "0.00118205\n",
      "0.00182567\n",
      "Epochs:23\n",
      "0.00176618\n",
      "0.00106908\n",
      "0.00115673\n",
      "Epochs:24\n",
      "0.0015574\n",
      "0.00193799\n",
      "0.0014927\n",
      "Epochs:25\n",
      "0.00117877\n",
      "0.00098049\n",
      "0.00108013\n",
      "Epochs:26\n",
      "0.00088026\n",
      "0.00146221\n",
      "0.00127244\n",
      "Epochs:27\n",
      "0.000871036\n",
      "0.00123242\n",
      "0.00126017\n",
      "Epochs:28\n",
      "0.000912925\n",
      "0.000902034\n",
      "0.000985192\n",
      "Epochs:29\n",
      "0.00200019\n",
      "0.00110387\n",
      "0.000973922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    epochs = 0\n",
    "    print_freq = 15\n",
    "    while epochs < 30:\n",
    "        print \"Epochs:\" + str(epochs)\n",
    "        i = 0\n",
    "        while i < len(x_train):\n",
    "            samples = x_train[i:i+batch_size,:]\n",
    "            i += batch_size\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={x:samples, y:samples})\n",
    "            if i % print_freq == 0:\n",
    "                print loss_val\n",
    "        epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SimpleSeq2Seq(output_dim=embedding_size, output_length=time_steps, input_shape=(time_steps, embedding_size))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x_train_embedded, x_train_embedded, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 2\n",
    "\n",
    "output_length = 8\n",
    "output_dim = 3\n",
    "\n",
    "samples = 100\n",
    "\n",
    "x = np.random.random((samples, input_length, input_dim))\n",
    "y = np.random.random((samples, output_length, output_dim))\n",
    "\n",
    "lookup_matrix\n",
    "\n",
    "models = []\n",
    "models += [SimpleSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim))]\n",
    "models += [SimpleSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim), depth=2)]\n",
    "\n",
    "for model in models:\n",
    "    model.compile(loss='mse', optimizer='sgd')\n",
    "    model.fit(x, y, nb_epoch=1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='sgd')\n",
    "model.fit(x_train, x_train, nb_epoch=1, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 2\n",
    "\n",
    "output_length = 8\n",
    "output_dim = 3\n",
    "\n",
    "samples = 100\n",
    "\n",
    "model = AttentionSeq2Seq(output_dim=output_dim, output_length=output_length, input_shape=(input_length, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(batch_size, time_steps))\n",
    "x = Embedding(input_dim=lookup_matrix.shape[0], output_dim=lookup_matrix.shape[1], weights=[lookup_matrix])(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "h_encoded = LSTM(hidden_dim, batch_input_shape=(batch_size, time_steps, embedding_size), \\\n",
    "                     return_sequences=False, stateful=True)(x)\n",
    "\n",
    "z_mean = Dense(latent_dim)(h_encoded)\n",
    "z_log_var = Dense(latent_dim)(h_encoded)\n",
    "\n",
    "# # build a model to project inputs on the latent space\n",
    "# encoder = Model(x, z_mean)\n",
    "\n",
    "# # display a 2D plot of the digit classes in the latent space\n",
    "# x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# # build a digit generator that can sample from the learned distribution\n",
    "# decoder_input = Input(shape=(latent_dim,))\n",
    "# _h_decoded = decoder_h(decoder_input)\n",
    "# _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "# generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# # display a 2D manifold of the digits\n",
    "# n = 15  # figure with 15x15 digits\n",
    "# digit_size = 28\n",
    "# figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# # to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "# grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "# grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "# for i, yi in enumerate(grid_x):\n",
    "#     for j, xi in enumerate(grid_y):\n",
    "#         z_sample = np.array([[xi, yi]])\n",
    "#         x_decoded = generator.predict(z_sample)\n",
    "#         digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "#         figure[i * digit_size: (i + 1) * digit_size,\n",
    "#                j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(figure, cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args         \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "h_0 = Dense(hidden_dim, activation='relu')(z)\n",
    "h_decoded = LSTM(embedding_size, batch_input_shape=(batch_size, time_steps, hidden_dim), \\\n",
    "                     return_sequences=True, stateful=True)(h_0)\n",
    "\n",
    "x_decoded_mean = Dense(original_dim, activation='sigmoid')(h_decoded)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(36,))\n",
    "x = Embedding(input_dim=lookup_matrix.shape[0], output_dim=lookup_matrix.shape[1], weights=[lookup_matrix])(input_tensor)\n",
    "\n",
    "model = Model(input=[input_tensor], output=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess.run(init)\n",
    "\n",
    "temp = sess.run(h_encoded, feed_dict={model.input:padded_idxes[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, LSTM\n",
    "\n",
    "batch_size = 10\n",
    "time_steps = 36\n",
    "embedding_size = 300\n",
    "\n",
    "h_encoded = LSTM(10, input_shape=(time_steps, embedding_size), \\\n",
    "                     return_sequences=True, stateful=False)(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
