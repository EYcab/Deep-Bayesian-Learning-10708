{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "\n",
    "# This defines the inner LSTM\n",
    "n_all_words = 100\n",
    "embedding_size = 30\n",
    "h_units_words = 40\n",
    "h_units_sentences = 50\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "n_sentences = 5\n",
    "# how many words in one sentence\n",
    "n_words = 10\n",
    "\n",
    "EOS = 1\n",
    "PAD = 0\n",
    "\n",
    "# This defines the embedding matrix\n",
    "lookup_matrix = np.zeros((n_all_words, embedding_size))\n",
    "\n",
    "inner_lstm = Sequential()\n",
    "inner_lstm.add(Embedding(input_dim=lookup_matrix.shape[0], \\\n",
    "    output_dim=lookup_matrix.shape[1], input_length=n_words, weights=[lookup_matrix]))\n",
    "inner_lstm.add(LSTM(h_units_words, return_sequences=True, input_shape=(None, h_units_words)))\n",
    "\n",
    "# This defines the outer LSTM\n",
    "outer_lstm = Sequential()\n",
    "# The embedding size here is the n_units_words from the inner LSTM\n",
    "outer_lstm.add(LSTM(h_units_sentences, return_sequences=False, input_shape=(None, h_units_words)))\n",
    "\n",
    "'''\n",
    "Below is the nested LSTM\n",
    "'''\n",
    "\n",
    "'''\n",
    "Encoder phase\n",
    "'''\n",
    "\n",
    "# (batch_size, n_words) is for inner lstm,\n",
    "# after embedding (batch_size, n_words, embedding_size)\n",
    "# after lstm, if use return_sequences=True, the output should has shape (batch_size, n_words, h_units_words)\n",
    "convs_placeholder = tf.placeholder(tf.float32, [None, 5, n_words])\n",
    "\n",
    "convs_placeholder_trans = tf.transpose(convs_placeholder, [1,0,2])\n",
    "\n",
    "# (n_sentence, batch_size, n_words, h_units_words)\n",
    "inner_outputs = tf.map_fn(lambda x:inner_lstm(x), convs_placeholder_trans)\n",
    "# (batch_size, n_sentence, n_words, h_units_words)\n",
    "inner_outputs_trans = tf.transpose(inner_outputs, [1,0,2,3])\n",
    "# sum out the third dimension for the input of outer lstm\n",
    "outer_lstm_input = tf.reduce_sum(inner_outputs_trans, axis=2)\n",
    "\n",
    "#  The encoded state to initialize the dynamic_rnn_decoder\n",
    "encoder_state = outer_lstm(outer_lstm_input)\n",
    "\n",
    "# attention state for the use of apply attention to the decoder [batch_size, n_words, h_units_words]\n",
    "attention_states = inner_outputs_trans[:,-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder phase\n",
    "'''\n",
    "\n",
    "# Below I basically copied from the tutorial\n",
    "\n",
    "decoder_hidden_units = 60\n",
    "\n",
    "(attention_keys,\n",
    "attention_values,\n",
    "attention_score_fn,\n",
    "attention_construct_fn) = seq2seq.prepare_attention(\n",
    "    attention_states=attention_states,\n",
    "    attention_option=\"bahdanau\",\n",
    "    num_units=decoder_hidden_units,\n",
    ")\n",
    "\n",
    "# attention is added\n",
    "decoder_fn_train = seq2seq.attention_decoder_fn_train(\n",
    "    encoder_state=encoder_state,\n",
    "    attention_keys=attention_keys,\n",
    "    attention_values=attention_values,\n",
    "    attention_score_fn=attention_score_fn,\n",
    "    attention_construct_fn=attention_construct_fn,\n",
    "    name='attention_decoder'\n",
    ")\n",
    "\n",
    "decoder_fn_inference = seq2seq.attention_decoder_fn_inference(\n",
    "    output_fn=output_fn,\n",
    "    encoder_state=self.encoder_state,\n",
    "    attention_keys=attention_keys,\n",
    "    attention_values=attention_values,\n",
    "    attention_score_fn=attention_score_fn,\n",
    "    attention_construct_fn=attention_construct_fn,\n",
    "    embeddings=lookup_matrix,\n",
    "    start_of_sequence_id=EOS,\n",
    "    end_of_sequence_id=EOS,\n",
    "    maximum_length = n_words + 3,\n",
    "    # vocabulary size\n",
    "    num_decoder_symbols=n_all_words,\n",
    ")\n",
    "\n",
    "\n",
    "(decoder_outputs_train, decoder_state_train, decoder_context_state_train) = seq2seq.dynamic_rnn_decoder(\n",
    "                                                        cell=self.decoder_cell,\n",
    "                                                        decoder_fn=decoder_fn_train,\n",
    "                                                        inputs=self.decoder_train_inputs_embedded,\n",
    "                                                        sequence_length=self.decoder_train_length,\n",
    "                                                        time_major=True,\n",
    "                                                        scope=scope)\n",
    "\n",
    "self.decoder_logits_train = output_fn(self.decoder_outputs_train)\n",
    "self.decoder_prediction_train = tf.argmax(self.decoder_logits_train, axis=-1, name='decoder_prediction_train')\n",
    "\n",
    "scope.reuse_variables()\n",
    "\n",
    "(decoder_logits_inference,\n",
    "decoder_state_inference,\n",
    "decoder_context_state_inference) = (\n",
    "seq2seq.dynamic_rnn_decoder(\n",
    "    cell=self.decoder_cell,\n",
    "    decoder_fn=decoder_fn_inference,\n",
    "    time_major=True,\n",
    "    scope=scope,\n",
    ")\n",
    ")\n",
    "\n",
    "self.decoder_prediction_inference = tf.argmax(self.decoder_logits_inference, axis=-1, name='decoder_prediction_inference')\n",
    "\n",
    "\n",
    "'''\n",
    "For Debugging\n",
    "'''\n",
    "\n",
    "# conversations variable is used for debugging use\n",
    "# every time the conversation fed in must be in the same number of sentences\n",
    "# It contains the input sentences and the sentence to be predicted\n",
    "conversations = np.zeros((batch_size, n_sentences, n_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
