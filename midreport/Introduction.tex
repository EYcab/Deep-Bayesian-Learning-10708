Conversation modeling is a famous task that allows the machine to generate reasonable responses according to the sentence it is shown. Previously, a fair amount of works havs been done.

In this project, we plan to improve the model performance based on previous works by incorporating latent information in the model by discovering several existing in variational methods. Especially, we are interested in RNN based variational autoencoder (VAE), that can seamlessly concatenate the seq2seq model with fine-tuned regularization.

By the time we write this midway report, we have produced some preliminary result of our vanilla seq2seq model. We've also implemented the variational auto-encoder, which we have yet not able to integrate the result into our proposed model.