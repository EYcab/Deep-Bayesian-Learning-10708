%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2017}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2017}

\begin{document} 

\twocolumn[
\icmltitle{Variational Neural Conversational Model}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chaoming Yen}{equal,to}
\icmlauthor{Yikang Li}{equal,to}
\icmlauthor{Xupeng Tong}{equal,to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Carnegie Mellon University, USA}

\icmlcorrespondingauthor{Xupeng Tong}{xtong@andrew.cmu.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%\footnotetext{hi}

\begin{abstract} 
Among all natural language understanding and machine intelligence tasks, conversational modeling is an important .

In this proposal, we describe a way to integrate variational inference to the attention matrix that might improve the performance of this task. 
\end{abstract} 

\section{Introduction}
Conversation modeling is a famous task that allows machine to generate reasonable responses according to the sentence it is shown. Previously, fair amount of works have done.

In this project, we plan to improve the model performance based on previous works .

To be added......



\section{Related Works}

\subsection{Neural Conversational Model}

Sequence To Sequence model is first introduced in \cite{seq2seq}, and since then, has become the standard model for dialogue systems and machine translation. It consists of two RNNs (Recurrent Neural Network) : An Encoder and a Decoder. The encoder takes a words sequence as input and processes one word at each time step. The objective is to convert symbol sequence into a fixed size feature vector that encodes the important information in the sequence while losing the redundant or unneccesary information.



\cite{ncm}
\cite{seq2seq}

\subsection{Auto-Encoding Variational Bayes}
\cite{vae}

Given an observed variable $x$, VAE introduces a continuous latent variable z, and assumes that $x$ is generated from $z$
$$p(x,z) = p(x|z)p(z)$$
The prior over the latent random variables, $p(z)$, is always chosen to be a simple Gaussian distribution and the conditional $p(x|z)$ is an arbitrary observation model whose parameters are computed by a parametric function of z. 
In VAE, $p(x|z)$ is typically parameterized function approximator such as a neural network. While latent random variable models of the form given in Eq. (3) are not uncommon, endowing the conditional $p(x|z)$ as a potentially highly non-linear mapping from z to x is a rather unique feature of the VAE.

The generative model $p(x|z)$ and inference model $q(z|x)$ are trained jointly by maximizing the variational lower bound with respect to their parameters, where the integral with respect to $q(z|x)$ is approximated stochastically. The gradient of this estimate can have a low variance estimate, by reparametrizing $z = \mu+\sigma\odot\epsilon$

$$ELBO_i (\lambda) = E_{q\lambda (z|x_i)}[\log p(x_i|z)]-KL(q\lambda (z|x_i)||p(z))$$ 
To be added and elaborated......


\subsection{Variational Recurrent Neural Network}
\cite{vrnn} 
To be added......

\subsection{Generative Adversarial Network} 
\cite{gan} 
 
\cite{ganncm} 
To be added...... 

\section{Datasets}

We will tested our model on the OpenSubtitles
dataset \cite{opensubtitle}. This is a dataset that incorporated movie subtitles with sentences uttered by characters.
The model will be trained to predict the next sentence given the previous one, for every consecutive sentences, and each sentence will be used both for context and as target.

 

\section{Plan}

\subsection{Incorporating latent variables in the training of Seq2Seq model}
There are various ways that allow us to improve our model with seq2seq 

Following the work in \cite{vnmt}, which introduces a variational model for neural machine translation that incorporates a continuous latent variable $z$ to model the underlying semantics of sentence pairs, we can also apply it to our neural conversation model that uses the same seq2seq model. 

We can also apply the method proposed by \cite{vrnn}, that explicitly models the dependencies between latent random variables across subsequent timesteps.

\subsection{Incorporating latent information unsupervisely as the input to Seq2Seq model}
Since sometimes, incorporating the latent variable into the training process directly may be hard. We consider an alternative approach that, instead of learn the latent variable through an end to end one way pass method. We can train a Variational Recurrent Auto-Encoder \cite{vrae} for each sentences first. VRAE is a variational autoencoder that can be used for the unsupervised learning on time series data, mapping the time series data to a latent vector representation. 

By appending the latent vector representation of each sentences along with the vector encoded by the seq2seq encoder, we naturally incorporate latent information of the sentence and that could serve as the input to be fed into the decoder phase.

\subsection{Improving the attention alignment model by variational inference }

One potential issue with this seq2seq model is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. To allow the decoder access to the input more directly, an attention mechanism was introduced in \cite{attention}.

The affect of the alignment model has become one of the most important feature of state-of-art sequence to sequence models. By incorporating latent variables in this particular part through variational method may gives us a boost in the model performance.

\subsection{Adversarial Variational Inference}
Improving the above mentioned variational method with Generic Adversarial Network (GAN) as proposed in \cite{avb}

\bibliography{refs}
\bibliographystyle{icml2017}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
